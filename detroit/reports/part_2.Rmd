---
title: "Part 2"
author: "Alexis Kwan"
output: 
  html_document:
    code_folding: hide
    df_print: paged
    theme: sandstone
---

```{r include=FALSE, eval=F}
# for use in part 2
# devtools::install_github("tidymodels/discrim")
install.packages("discrim")
install.packages("parsnip")
install.packages("naivebayes")
install.packages("DT")
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
library(parsnip)
library(tidyverse)
library(tidymodels)
library(kableExtra)
library(lubridate)
library(discrim)
library(cmfproperty)
library(lubridate)

con <- DBI::dbConnect(RSQLite::SQLite(), "../database/detroit.sqlite")
```

```{r include=FALSE}
assessments <- dplyr::tbl(con, 'assessments') %>% collect()
sales <- dplyr::tbl(con, 'sales') %>% collect()
parcels <- dplyr::tbl(con, 'parcels') %>% collect()

foreclosures <- dplyr::tbl(con, 'foreclosures') %>%
  collect() %>%
  select(-prop_addr) %>%
  pivot_longer(!prop_parcelnum, names_to='year', values_to='foreclosed') %>%
  filter(!is.na(foreclosed)) %>%
  distinct()

joined = 
  sales %>%
  mutate(year = year(ymd(sale_date)),
         sale_year = as.factor(year)) %>%
  left_join(assessments, by=c('parcel_num'='PARCELNO', c('year'='year')), suffix = c("", "_assessed")) %>%
  left_join(parcels, by = c('parcel_num'='parcel_number'), suffix = c("", "_parcel")) %>%
  mutate(property_c = as.character(property_c),
         propclass = as.character(propclass)) %>% 
  filter(str_detect(sale_terms, regex("valid arm", ignore_case = TRUE))) %>%
  filter(propclass == 401)

joined_mini = 
  joined %>%
  filter(property_c == 401,
         sale_price >= 2000,
         ASSESSEDVALUE >= 1000,
         str_detect(str_to_lower(sale_terms), 'valid arm')) %>%
  left_join(foreclosures, by=c('parcel_num'='prop_parcelnum', c('sale_year'='year')))

ratios =
  cmfproperty::reformat_data(
    joined_mini,
    sale_col = "sale_price",
    assessment_col = "ASSESSEDVALUE",
    sale_year_col = "year",
  )

stats = calc_iaao_stats(ratios)
```

# Part 2: Classification

## Part A: Introduction

Under the current foreclosure crisis in Detroit that has been going on since 2011 it is pertinent that properties be assessed properly so as to not deepen the crisis but still maintain equity and revenue streams. Since previous studies of assessments have been determined to be inequitable but the the assessment quality, based on the sales ratio, has improved over time as shown in the figure below where the zero line represents 50% of the market value.

```{r}
stats %>%
  mutate(ratio_diff = median_ratio-0.5,
         over_under = ifelse(ratio_diff > 0, "Under assessed", "Over assessed")) %>%
  ggplot() +
  geom_segment(aes(x=Year, xend=Year, y=0, yend=ratio_diff, color=over_under), size=10, alpha=0.9) +
  theme_light() +
  ylim(-0.5, 0.5) +
  scale_x_continuous(breaks = pretty_breaks()) +
  labs(title = "Median Sales Ratio 2011-2020",
       y = "Sales Ratio Difference",
       color = "")
```

```{r}
library(ggridges)

joined_mini %>%
  ggplot(aes(y=sale_year, x=sale_price, fill=sale_year)) +
  geom_boxplot() +
  theme(legend.position="none") +
  labs(title = "Sales 2011-2020",
       x = "Sale Price",
       y = "Year") +
  scale_x_log10(labels = dollar)
  # scale_x_continuous(breaks = pretty_breaks())
```

```{r}
joined_mini %>%
  filter(!is.na(foreclosed)) %>%
  group_by(sale_year) %>%
  count(foreclosed) %>%
  ggplot(aes(y=sale_year, x=n)) +
  geom_col() +
    xlab("Foreclosures") +
    ylab("Year")
```

## Part B: Overassessment Classification

First, we want to find a way to identify if a home is likely to be overassessed in a given year. We will analyze homes and assessments from 2016. Use 2016 sales and assessments with the parcels property characteristics (note that we only know if a home was overassessed if it sold). Create a classification metric of overassessment based on properties which sold and use this as your dependent variable.

```{r overassessment workflow}
nb_model = 
  naive_Bayes() %>%
  set_mode("classification") %>%
  set_engine("naivebayes") %>%
  set_args(smoothness = .5)

over_workflow = 
  workflow() %>%
  add_model(nb_model)

ratios_2016 =
  ratios %>%
  mutate(overassessed = as.factor(if_else(RATIO > 0.5, 'Overassesed', 'Not overassessed'))) %>%
  filter(sale_year == "2016")

over_recipe =
  recipe(overassessed ~ zip_code + property_class_desc + use_code + 
           total_square_footage + year_built + is_improved, 
         data=ratios_2016) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors())

over_workflow =
  over_workflow %>%
  add_recipe(over_recipe)

ratios_split = initial_split(ratios_2016)

ratios_train = training(ratios_split)
ratios_test = testing(ratios_split)

nb_fit = over_workflow %>%
  fit(data=ratios_train)

cmat = augment(nb_fit, new_data = ratios_test) %>% 
  conf_mat(truth = overassessed, estimate = .pred_class)

over_pred = predict(nb_fit, ratios_test, type = "prob")

cmat
```

```{r}
# cs = 
summary(cmat) %>%
  tibble() %>%
  rename_all(~ toupper(gsub(".", "", .x, fixed = TRUE))) 

# cs %>%
#   datatable()
```

```{r}
# factor levels are in alphabetical order so roc_curve assumes that 'Not overassessed' is the positive case
autoplot(roc_curve(over_pred, ratios_test$overassessed, ".pred_Not overassessed", na_rm = T)) +
  labs(title = 'ROC Curve for Identification of Overassessments',
       subtitle = 'Using Naive Bayes')
```

## Part C: 2019 Assessments

```{r}
joined_mini_pre2019 =
  joined_mini %>%
  filter(year < 2019)

joined_mini_2019 =
  joined_mini %>%
  filter(year == 2019)

assess_model <-
  decision_tree() %>%
  set_engine("rpart") %>%
  set_mode('regression')

assess_recipe <- recipe(sale_price ~
                         ecf + total_square_footage + year + use_code 
                        + is_improved + zoning + zip_code, 
                       data = joined_mini_pre2019) %>%
  step_filter(!is.na(sale_price)) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors())

assess_workflow <-
  workflow() %>%
  add_model(assess_model) %>%
  add_recipe(assess_recipe)

a_split = initial_split(joined_mini_pre2019)

a_train = training(a_split)
a_test = testing(a_split)

model_fit <- assess_workflow %>%
  fit(data=a_train)

multi_metric = metric_set(rmse, mape)

tree_metrics = augment(model_fit, new_data = a_test) %>% 
  multi_metric(truth = sale_price, estimate = .pred)

tree_metrics %>%
  rename_all(~ toupper(gsub(".", "", .x, fixed = TRUE))) 
```

